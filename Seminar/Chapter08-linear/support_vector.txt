SVM
1.導入
サポートベクタマシン(support vector machines)はニクラス分類の機械学習モデルであり、その目的はデータセットを分割するための超平面を探すことである。分割の原則条件はマージン最大化であり、もとの問題を最終的に凸二次計画問題に転換して解を求める。問題転換のモデルとしては以下のようなものがある。
（１）データセットが線形分離可能のとき、ハードマージン・サポートベクターマシンを通して線形モデルのサポートベクターマシンを学習する。
（２）データセットが近似線形分離可能なとき、ソフトマジン・サポートベクターマシンを通して線形モデルのサポートベクターマシンを学習する。
（３）データセットが線形分離不可能のとき、カーネルトリックとソフトマージンサポートベクタマシンを通して非線形モデルのサポートベクターマシンを学習する。

2.線形モデルによるサポートベクターマシン
以下のデータを学習データとして与える。（データ内容）クラス分類学習の基本的な考え方はデータセットに基づいて標本空間の中にこれらのデータを分類できる超平面を求めることである。
直感的にわかるとおり、データセットを分割できる超平面は数多く存在する。ただし、クラス分類をするための超平面は、クラス間における「ちょうど真ん中」超平面を選ぶ必要がある。なぜなら、クラス間の「ちょうど真ん中」を通過する超平面はデータセットの局部的な変動に対しての「受容性」が最も優れているからである。例えば、データセットの局所性や騒音などの誤差の影響により、訓練データ以外のデータが図6.1よりもクラス分類を行う超平面の近くに存在している可能性がある。このようなことは超平面によるクラス分類における誤分類の原因となる。しかし、マージンが最大になっている超平面はこのような影響による誤分類の可能性を最小に抑えることができる。簡単に言うと、マージン最大によるサポートベクターマシンによる分類結果はロバストであると言える。
それでは、線形分離可能とはどういうことか。
もし線形な関数によってデータセットを分けるとこができれば、このデータは線形分離可能と言える。それでは線形な関数とは何か。2次空間や3次空間で考えれば、線形な関数はそれぞれ直線と平面という形で可視化することができる。このように類推することにより、もしベクトル空間の次元を考慮に入れなければ、n次元における線形な関数を超平面として見なすことができる。ここでは2次空間における簡単な例を挙げる。○は正のクラス、✕は負のクラスとし、個々におけるデータセットは線形分離可能なものとする。しかしここでは正・負のクラス分けを行うための直線が無数個存在することは明らかである。先程述べた線形分離可能なサポートベクターマシンはこの例におけるマージンを最大にする直線のことである。
（図）
ここで我々が考えなければならないことは、なぜマージンを最大にしなければならないことである。

